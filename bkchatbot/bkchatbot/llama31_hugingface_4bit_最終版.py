import os
import re
import sys
import logging
import tempfile
from dotenv import load_dotenv
from flask import Flask, request, abort
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.webhooks import MessageEvent, TextMessageContent, AudioMessageContent
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    MessagingApiBlob,
    ReplyMessageRequest,
    TextMessage,
)
from qdrant_client import QdrantClient
from langchain_openai import OpenAIEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
from openai import OpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
logging.basicConfig(level=logging.INFO)
load_dotenv()

# è®€å–ç’°å¢ƒè®Šæ•¸
api_key = os.getenv('OPENAI_API_KEY')
qdrant_url = os.getenv('QDRANT_URL')
qdrant_api_key = os.getenv('QDRANT_API_KEY')
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
channel_secret = os.getenv('LINE_CHANNEL_SECRET')

# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆç”¨æ–¼èªéŸ³è½‰æ–‡å­—ï¼‰
whisper_client = OpenAI(api_key=api_key)

# åˆå§‹åŒ– OpenAIEmbeddingsï¼ˆç”¨æ–¼å‘é‡æŸ¥è©¢ï¼‰
embeddings = OpenAIEmbeddings(
    api_key=api_key,
    model="text-embedding-3-small",
)

# åˆå§‹åŒ– QdrantClient
qdrant_client = QdrantClient(
    url=qdrant_url,
    api_key=qdrant_api_key,
)

# è¨­å®š LLaMA 3.1 Instruct æ¨¡å‹
model_name = "meta-llama/Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)

# åˆå§‹åŒ– Flask æ‡‰ç”¨
app = Flask(__name__)

if channel_secret is None or channel_access_token is None:
    print('Please specify LINE_CHANNEL_SECRET and LINE_CHANNEL_ACCESS_TOKEN as environment variables.')
    sys.exit(1)

handler = WebhookHandler(channel_secret)
configuration = Configuration(access_token=channel_access_token)

@app.route("/callback", methods=['POST'])
def callback():
    """LINE è¨Šæ¯å›æ‡‰ç«¯é»"""
    signature = request.headers.get('X-Line-Signature')
    body = request.get_data(as_text=True)
    app.logger.info(f"Request body: {body}")
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

# ä½¿ç”¨ OpenAI Whisper é€²è¡ŒèªéŸ³è½‰æ–‡å­—
def speech_to_text(audio_path):
    try:
        with open(audio_path, "rb") as audio_file:
            transcription = whisper_client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="zh"  # æŒ‡å®šèªè¨€ç‚ºä¸­æ–‡
            )
        transcript = transcription.text.strip()
        logging.info(f"Transcript: {transcript}")
        return transcript
    except Exception as e:
        logging.error(f"Error processing audio file with OpenAI Whisper: {e}")
        return None

# è™•ç†éŸ³é »æ¶ˆæ¯äº‹ä»¶
@handler.add(MessageEvent, message=AudioMessageContent)
def handle_audio_message(event):
    with ApiClient(configuration) as api_client:
        line_bot_blob_api = MessagingApiBlob(api_client)
        audio_data = line_bot_blob_api.get_message_content(
            message_id=event.message.id
        )

        with tempfile.NamedTemporaryFile(suffix='.m4a', delete=False) as temp_audio:
            temp_audio.write(audio_data)
            temp_audio_path = temp_audio.name

        transcript = speech_to_text(temp_audio_path)
        if transcript:
            # ä½¿ç”¨å¾èªéŸ³è­˜åˆ¥å¾—åˆ°çš„æ–‡å­—é€²è¡Œå›è¦†ç”Ÿæˆ
            reply_message = generate_rag_response(transcript, event.source.user_id)
        else:
            reply_message = "ç„¡æ³•è¾¨è­˜èªéŸ³å…§å®¹"

        line_bot_api = MessagingApi(api_client)
        line_bot_api.reply_message(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text=reply_message)],
            )
        )

        os.remove(temp_audio_path)


@handler.add(MessageEvent, message=TextMessageContent)
def message_text(event):
    """è™•ç† LINE å‚³ä¾†çš„æ–‡å­—è¨Šæ¯"""
    user_input = event.message.text
    user_id = event.source.user_id

    reply_message = generate_rag_response(user_input, user_id)

    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        line_bot_api.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text=reply_message)]
            )
        )

def generate_rag_response(user_input, user_id):
    """ä½¿ç”¨ RAG + LLaMA 3.1 ç”¢ç”Ÿå›æ‡‰"""
    return query_rag_llama3(user_input, user_id)

import re

def query_rag_llama3(user_message, session_id):
    """åŸ·è¡Œå‘é‡æª¢ç´¢ + LLaMA 3.1 ç”¢ç”Ÿå›æ‡‰"""
    
    # 1ï¸âƒ£ Qdrant å‘é‡æª¢ç´¢
    query_vector = embeddings.embed_query(user_message)
    search_results = qdrant_client.search(
        collection_name="250206-PDF-3-small-final",
        query_vector=query_vector,
        limit=1
    )

    search_results_description = "\n".join([
        f"æ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶: {doc.payload['page_content']}"
        for doc in search_results]) if search_results else "æœªæ‰¾åˆ°èˆ‡æŸ¥è©¢ç›¸é—œçš„è³‡è¨Šã€‚"

    print(f"ğŸ” å‘é‡è³‡æ–™æœå°‹çµæœ: {search_results_description}")

    # 2ï¸âƒ£ **ç¬¦åˆ LLaMA 3.1 Instruct æ ¼å¼**
    prompt_text = f"""<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI åŠ©ç†ï¼Œå°ˆé–€è§£ç­”åœŸåœ°éŠ€è¡Œæ¨‚æ´»é¤Šè€æˆ¿å±‹å°ˆæ¡ˆçš„å•é¡Œã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šæä¾›æº–ç¢ºçš„å›æ‡‰ï¼Œä¸¦ä½¿ç”¨æ¢åˆ—å¼å›ç­”ï¼Œå›æ‡‰ä¸è¶…é 100 å­—ã€‚
<|eot_id|>

<|start_header_id|>user<|end_header_id|>
æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š
{search_results_description}

ä½¿ç”¨è€…å•é¡Œï¼š{user_message}
<|eot_id|>

<|start_header_id|>assistant<|end_header_id|>
"""

    # 3ï¸âƒ£ **ä½¿ç”¨ LLaMA 3.1 ç”Ÿæˆå›æ‡‰**
    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=1024).to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,   # é™åˆ¶æœ€å¤š 150 å€‹ token
            temperature=0.3,      # é™ä½éš¨æ©Ÿæ€§ï¼Œç¢ºä¿å›ç­”ç©©å®š
            repetition_penalty=1.1,
            do_sample=False       # é—œé–‰éš¨æ©Ÿå–æ¨£ï¼Œæé«˜ä¸€è‡´æ€§
        )

    final_reply = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    final_reply = final_reply.split("assistant", 1)[-1].strip(": \n")

    return final_reply

if __name__ == "__main__":
    app.run()
