import os
import re
import sys
import logging
import tempfile
from dotenv import load_dotenv
from flask import Flask, request, abort
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.webhooks import MessageEvent, TextMessageContent, AudioMessageContent
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    MessagingApiBlob,
    ReplyMessageRequest,
    TextMessage,
)
from qdrant_client import QdrantClient
from langchain_openai import OpenAIEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
from openai import OpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
logging.basicConfig(level=logging.INFO)
load_dotenv()

# è®€å–ç’°å¢ƒè®Šæ•¸
api_key = os.getenv('OPENAI_API_KEY')
qdrant_url = os.getenv('QDRANT_URL')
qdrant_api_key = os.getenv('QDRANT_API_KEY')
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
channel_secret = os.getenv('LINE_CHANNEL_SECRET')

# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆç”¨æ–¼èªéŸ³è½‰æ–‡å­—ï¼‰
whisper_client = OpenAI(api_key=api_key)

# åˆå§‹åŒ– OpenAIEmbeddingsï¼ˆç”¨æ–¼å‘é‡æŸ¥è©¢ï¼‰
embeddings = OpenAIEmbeddings(
    api_key=api_key,
    model="text-embedding-3-small",
)

# åˆå§‹åŒ– QdrantClient
qdrant_client = QdrantClient(
    url=qdrant_url,
    api_key=qdrant_api_key,
)

# è¨­å®š LLaMA 3.1 Instruct æ¨¡å‹
model_name = "meta-llama/Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)

# åˆå§‹åŒ– Flask æ‡‰ç”¨
app = Flask(__name__)

if channel_secret is None or channel_access_token is None:
    print('Please specify LINE_CHANNEL_SECRET and LINE_CHANNEL_ACCESS_TOKEN as environment variables.')
    sys.exit(1)

handler = WebhookHandler(channel_secret)
configuration = Configuration(access_token=channel_access_token)

@app.route("/callback", methods=['POST'])
def callback():
    """LINE è¨Šæ¯å›æ‡‰ç«¯é»"""
    signature = request.headers.get('X-Line-Signature')
    body = request.get_data(as_text=True)
    app.logger.info(f"Request body: {body}")
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

@handler.add(MessageEvent, message=TextMessageContent)
def message_text(event):
    """è™•ç† LINE å‚³ä¾†çš„æ–‡å­—è¨Šæ¯"""
    user_input = event.message.text
    user_id = event.source.user_id

    reply_message = generate_rag_response(user_input, user_id)

    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        line_bot_api.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text=reply_message)]
            )
        )

def generate_rag_response(user_input, user_id):
    """ä½¿ç”¨ RAG + LLaMA 3.1 ç”¢ç”Ÿå›æ‡‰"""
    return query_rag_llama3(user_input, user_id)

def query_rag_llama3(user_message, session_id):
    """åŸ·è¡Œå‘é‡æª¢ç´¢ + LLaMA 3.1 ç”¢ç”Ÿå›æ‡‰"""
    
    # 1ï¸âƒ£ Qdrant å‘é‡æª¢ç´¢
    query_vector = embeddings.embed_query(user_message)
    search_results = qdrant_client.search(
        collection_name="250206-PDF-3-small-final",
        query_vector=query_vector,
        limit=1
    )

    search_results_description = "\n".join([
        f"æ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶: {doc.payload['page_content']}"
        for doc in search_results]) if search_results else "æœªæ‰¾åˆ°èˆ‡æŸ¥è©¢ç›¸é—œçš„è³‡è¨Šã€‚"

    print(f"ğŸ” å‘é‡è³‡æ–™æœå°‹çµæœ: {search_results_description}")

    # 2ï¸âƒ£ **æ‰‹å‹•æ§‹å»ºç¬¦åˆ LLaMA 3.1 Instruct çš„ Prompt**
    prompt_text = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI åŠ©ç†ï¼Œå°ˆé–€è§£ç­”åœŸåœ°éŠ€è¡Œæ¨‚æ´»é¤Šè€æˆ¿å±‹å°ˆæ¡ˆçš„å•é¡Œã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šæä¾›æº–ç¢ºçš„å›æ‡‰ï¼Œä¸¦ä½¿ç”¨æ¢åˆ—å¼å›ç­”ï¼Œå›æ‡‰ä¸è¶…é 100 å­—ã€‚

æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š
{search_results_description}

ä½¿ç”¨è€…å•é¡Œï¼š{user_message}

å›ç­”ï¼š
"""

    # 3ï¸âƒ£ ä½¿ç”¨ LLaMA 3.1 ç”Ÿæˆå›æ‡‰
    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=1024).to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.3,
            repetition_penalty=1.1,
            do_sample=False  # ç¢ºä¿ç”Ÿæˆä¸€è‡´æ€§
        )

    final_reply = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # 4ï¸âƒ£ **ç§»é™¤ LLaMA å¯èƒ½å›æ‡‰çš„ Prompt å…§å®¹**
    final_reply = final_reply.replace(prompt_text, "").strip()

    return final_reply




if __name__ == "__main__":
    app.run()
